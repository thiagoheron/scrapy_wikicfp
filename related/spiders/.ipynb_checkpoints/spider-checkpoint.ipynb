{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import's\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"categorySC\"\n",
    "\n",
    "    start_urls = [\n",
    "        'http://www.wikicfp.com/cfp/call?conference=computer%20science',\n",
    "    ]\n",
    "        \n",
    "        \n",
    "    def parse(self, response):\n",
    "        \n",
    "        links_all_pages = []\n",
    "        first_page = 'http://www.wikicfp.com/cfp/call?conference=computer%20science'\n",
    "        \n",
    "        \n",
    "        # If is the second call of this function(recursive), then exists reponse.meta[link events]\n",
    "        if response.request.url != first_page:\n",
    "            for link in response.meta['links_all_pages']:\n",
    "                if link not in links_all_pages:\n",
    "                    links_all_pages.append(head)\n",
    "        \n",
    "                \n",
    "                \n",
    "        \n",
    "        # Get all links of each page of category science computer\n",
    "        contsec = response.css(\".contsec\")[0]\n",
    "        for i in range(8, len(contsec.css(\"tr\"))):\n",
    "            tr_correct = contsec.css(\"tr\")[i]\n",
    "            if tr_correct.css(\"td a\"):\n",
    "                tr_a = tr_correct.css(\"td a\")\n",
    "                link = \"http://www.wikicfp.com\" + tr_a.css(\"a::attr(href)\").extract()[0]\n",
    "                head, sep, tail = link.partition('&copyownerid=')\n",
    "                \n",
    "                if head not in links_all_pages:\n",
    "                    links_all_pages.append(head)\n",
    "                #links_all_pages.append('http://www.wikicfp.com'+ tr_a.css(\"a::attr(href)\").extract()[0])\n",
    "            \n",
    "        \n",
    "        print(\"\\nLink Page:\\n\")\n",
    "        # Check links all pages\n",
    "        for link in links_all_pages:\n",
    "            print(\"Link: {l}\".format(l=link))\n",
    "        \n",
    "        \n",
    "         \n",
    "        \n",
    "           \n",
    "        \n",
    "        \n",
    "        # Discover if is the first page or other. The first page has next page in index 1, and others index 2\n",
    "        size_menu = len(contsec.css(\"tr\")[(len((contsec.css(\"tr\")))-1)].css(\"a::attr(href)\"))\n",
    "        if size_menu == 3:\n",
    "            next_page = 'http://www.wikicfp.com' + contsec.css(\"tr\")[(len((contsec.css(\"tr\")))-1)].css(\"a::attr(href)\")[1].extract()\n",
    "        else:\n",
    "            next_page = 'http://www.wikicfp.com' + contsec.css(\"tr\")[(len((contsec.css(\"tr\")))-1)].css(\"a::attr(href)\")[2].extract()\n",
    "            \n",
    "    \n",
    "        # If the last page is different to next_page... end.\n",
    "        if response.request.url != next_page:\n",
    "            request = scrapy.Request(url=next_page, callback=self.parse, meta={'links_all_pages':links_all_pages})\n",
    "            yield request\n",
    "       \n",
    "        else:\n",
    "            for link in links_all_pages:\n",
    "                # Here make a request for each link of 21 pages science computer\n",
    "                request = scrapy.Request(url=link, callback=self.parse_related_links)\n",
    "                yield request\n",
    "        \n",
    "            \n",
    "       \n",
    "       \n",
    "    def parse_related_links(self, response):\n",
    "        \n",
    "        links_page_related = [] \n",
    "        div_related_resources = response.css(\".contsec\")[1].css(\".cfp tr td a\")\n",
    "        \n",
    "        # Get the links of related links area\n",
    "        for i in range(0, len(response.css(\".contsec\")[1].css(\".cfp tr td a\").extract())):\n",
    "                link = \"http://www.wikicfp.com\" + div_related_resources.css(\"a::attr(href)\")[i].extract()\n",
    "                head, sep, tail = link.partition('&copyownerid=')\n",
    "                if head not in links_all_pages:\n",
    "                    links_all_pages.append(head)\n",
    "              \n",
    "                #links_page_related.append(div_related_resources.css(\"a::attr(href)\")[i].extract())\n",
    "                #links_page_related[i] = \"http://www.wikicfp.com\" + links_page_related[i]\n",
    "                \n",
    "              \n",
    "        \n",
    "        print(\"Link Base: {l}\".format(l=response.request.url))\n",
    "        for link in links_page_related:\n",
    "            print(\"------Link Related: {l}\".format(l=link))\n",
    "            \n",
    "            \n",
    "        # Add the current link request\n",
    "        links_page_related.append(response.request.url)\n",
    "\n",
    "        \n",
    "        for link in links_page_related:\n",
    "                request = scrapy.Request(url=link, callback=self.parse_page)\n",
    "                yield request\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def parse_page(self, response): \n",
    "        datas = response.css(\".contsec\")[0]\n",
    "        table = datas.css(\"table .gglu\")[0]\n",
    "        values = []\n",
    "        \n",
    "        # Title\n",
    "        title = response.css(\"title::text\").extract_first()\n",
    "        \n",
    "        # Keys of Table\n",
    "        keys = table.css(\"tr th::text\").extract() \n",
    "        \n",
    "        # Valeus of Table\n",
    "        for i in range(0, len(keys)):\n",
    "            item = table.css(\"tr td\")[i]\n",
    "            if not item.css(\"span\"):\n",
    "                values.append(table.css(\"tr td::text\")[i].extract())\n",
    "            else:\n",
    "                t = table.css(\"tr td\")[i]\n",
    "                values.append(item.css(\"span::text\")[(len(item.css(\"span\"))-1)].extract())\n",
    "        \n",
    "        # Categories\n",
    "        tagCategories = response.css(\"tr td h5\")\n",
    "        categories = []\n",
    "        for i in range(0, len(tagCategories.css(\"a::text\"))):\n",
    "            categories.append(tagCategories.css(\"a::text\")[i].extract())\n",
    "        \n",
    "        # Event ID\n",
    "        contsec = response.css(\"form\")\n",
    "        eventid = contsec[1].css(\"input\")\n",
    "        id_event = eventid.css(\"input\").xpath(\"@value\")[1].extract()\n",
    "        \n",
    "        # Link of Event\n",
    "        try:\n",
    "            link_event = response.css(\".contsec tr\")[5].css(\"a::attr(href)\").extract()[0]\n",
    "        except IndexError:\n",
    "            link_event = 'null'\n",
    "        \n",
    "        # Related Resources\n",
    "        link_resource = []\n",
    "        name_resource = []\n",
    "        related_resources = response.css(\".contsec\")[1].css(\".cfp tr td a\")\n",
    "        for i in range(0, len(response.css(\".contsec\")[1].css(\".cfp tr td a\").extract())):\n",
    "                link_resource.append(related_resources.css(\"a::attr(href)\")[i].extract())\n",
    "                link_resource[i] = \"http://www.wikicfp.com\" + link_resource[i]\n",
    "                name_resource.append(related_resources.css(\"a::text\")[i].extract())\n",
    "                \n",
    "        \n",
    "        # Description\n",
    "        ''' \n",
    "        try:\n",
    "            div_cfp = response.css(\".cfp\")[0]\n",
    "            description = div_cfp.css(\"::text\").extract()\n",
    "        except IndexError:\n",
    "            description = 'null'\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create a dict from links of related resourcers 'title':'link'\n",
    "        dfResources = pd.Series(link_resource, index=name_resource)\n",
    "        dfResources = dfResources.to_dict()\n",
    "        \n",
    "        # Create a dict from Series and add new keys:values\n",
    "        for i in range(0, len(keys)):\n",
    "            keys[i] = keys[i].lower()\n",
    "            dfTable = pd.Series(values, index=keys) \n",
    "            dfTable = dfTable.to_dict()\n",
    "        dfTable['categories'] = categories\n",
    "        dfTable['title'] = title\n",
    "        dfTable['link_event'] = link_event\n",
    "        dfTable.update({'related_resources': dfResources})\n",
    "        #dfTable['description'] = description\n",
    "         \n",
    "    \n",
    "            \n",
    "        yield{\n",
    "            id_event: dfTable\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "teste = []\n",
    "teste.append(1)\n",
    "print(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste2 = []\n",
    "teste2.insert(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
